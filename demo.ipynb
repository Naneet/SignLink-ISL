{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344be695",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e53d32-3586-4119-b298-0511df798c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_and_words(path):\n",
    "    \"\"\"Use this function to load the data path and label for the dataset\"\"\"\n",
    "    words = os.listdir(path)\n",
    "    words_list = []\n",
    "    data = []\n",
    "    for word in words:\n",
    "        words_list.append(word)\n",
    "        word_videos_path = os.path.join(path, word)\n",
    "        videos = os.listdir(word_videos_path)\n",
    "        for video in videos:\n",
    "            data.append((os.path.join(word_videos_path, video), len(words_list)-1))\n",
    "\n",
    "    words_dict = dict({})\n",
    "    for num, word in enumerate(words_list):\n",
    "        words_dict[word] = num\n",
    "\n",
    "    return data, words_list, words_dict\n",
    "\n",
    "data, idx_to_word, word_to_idx = words_and_idx(\"path/to/dataset/folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d347b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import torchvision.transforms as transforms\n",
    "import shutil\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "# tag : v5.2\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data, temp_data_folder, NUM_FRAMES=10, transform_frame=None, video_fps=25, resolution='1920:1080', flip_prob=30, resize=300, crop=640):\n",
    "        self.data = data  # It should be a list of tuples where data[0] is the path to the video and data[1] is the label\n",
    "\n",
    "        self.transform_frame = transform_frame  # Transformations to be done on the individual frames.\n",
    "                                                # Recommended to use when transforms is required at frames level with some randomness, eg: Random Crop\n",
    "\n",
    "        self.NUM_FRAMES = NUM_FRAMES  # Number of frames to be extracted from the video\n",
    "\n",
    "        self.fps = video_fps  # The fps at which the video will be saved by ffmpeg\n",
    "                              # Note: Reducing this might give a small performance increase, which might add up when running it multiple times. But this will also lead to\n",
    "                              # loss of some data, as some frames will be dropped by ffmpeg\n",
    "\n",
    "        self.resolution = resolution  # resolution at which ffmpeg will save the frames (could be the same as the video or different).\n",
    "                                      # Note: Reducing this might give a small performance increase, which might add up when running it multiple times.\n",
    "\n",
    "        self.temp_data_folder = temp_data_folder  # A temporary folder where ffmpeg can store the frames of the video\n",
    "                                                  # NOTE: this folder is recommended to be empty as frames get deleted from the folder after they are loaded as tensor!!\n",
    "\n",
    "        self.flip_prob = flip_prob  # Probability of flipping a video\n",
    "\n",
    "        self.frame_width = np.fromstring(resolution, sep=\":\")[0]  # For converting the coordinates from 0-1 to desired resolution\n",
    "        self.frame_length = np.fromstring(resolution, sep=\":\")[1]\n",
    "\n",
    "        self.rescale = transforms.Resize((resize,resize))  # For reducing the resolution of the image after plotting the landmarks\n",
    "        self.crop = transforms.CenterCrop((crop,crop))     # Recommended to do in this way otherwise the model have difficulty generalising the dataset\n",
    "\n",
    "    def landmark_to_list(self, landmarks):  # converting mediapipe landmarks to python list\n",
    "        x, y =[], []\n",
    "        for landmark in landmarks:\n",
    "            x.append(landmark.x)\n",
    "            y.append(landmark.y)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def swap(self, left_thumb, right_thumb, hand):  # swapping hands side if it is assigned to wrong side\n",
    "        left = (left_thumb[0]-hand[0])**2 + (left_thumb[1]-hand[1])**2\n",
    "        right = (right_thumb[0]-hand[0])**2 + (right_thumb[1]-hand[1])**2\n",
    "\n",
    "        if right<left:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def process_and_convert(self, image_path, flip):\n",
    "\n",
    "        # Initialize MediaPipe modules\n",
    "        mp_pose = mp.solutions.pose\n",
    "        mp_hands = mp.solutions.hands\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "        mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "        # Load the image\n",
    "        rgb_image = Image.open(image_path)\n",
    "\n",
    "        with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose, \\\n",
    "            mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.1) as hands:\n",
    "\n",
    "            # Convert the BGR image to RGB\n",
    "            if rgb_image.mode != 'RGB':\n",
    "                rgb_image = rgb_image.convert('RGB')\n",
    "\n",
    "            transform = transforms.RandomHorizontalFlip(p=1)\n",
    "            if flip:\n",
    "                rgb_image = transform(rgb_image)\n",
    "\n",
    "            if self.transform_frame:\n",
    "                rgb_image = self.transform_frame(rgb_image)\n",
    "\n",
    "            rgb_image = np.array(rgb_image)\n",
    "\n",
    "            # Process pose landmarks\n",
    "            pose_results = pose.process(rgb_image)\n",
    "            x_pose, y_pose = self.landmark_to_list(pose_results.pose_landmarks.landmark[:25])\n",
    "\n",
    "\n",
    "            # Process hand landmarks\n",
    "            hand_results = hands.process(rgb_image)\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                if len(hand_results.multi_hand_landmarks) > 0 and hand_results.multi_hand_landmarks[0] != None:\n",
    "                    x_left_hand, y_left_hand = self.landmark_to_list(hand_results.multi_hand_landmarks[0].landmark)\n",
    "                else:\n",
    "                    x_left_hand, y_left_hand = [np.nan]*21, [np.nan]*21\n",
    "\n",
    "                if len(hand_results.multi_hand_landmarks) > 1 and hand_results.multi_hand_landmarks[1] != None:\n",
    "                    x_right_hand, y_right_hand = self.landmark_to_list(hand_results.multi_hand_landmarks[1].landmark)\n",
    "                else:\n",
    "                    x_right_hand, y_right_hand = [np.nan]*21, [np.nan]*21\n",
    "\n",
    "                    if self.swap(left_thumb=(x_pose[21], y_pose[21]), right_thumb=(x_pose[22], y_pose[22]), hand=(x_left_hand[1], y_left_hand[1])):\n",
    "                        x_left_hand, y_left_hand, x_right_hand, y_right_hand = x_right_hand, y_right_hand, x_left_hand, y_left_hand\n",
    "\n",
    "            else:\n",
    "                x_left_hand, y_left_hand = [np.nan]*21, [np.nan]*21\n",
    "                x_right_hand, y_right_hand = [np.nan]*21, [np.nan]*21\n",
    "\n",
    "\n",
    "            return x_left_hand, y_left_hand, x_pose, y_pose, x_right_hand, y_right_hand\n",
    "\n",
    "\n",
    "    def interpolate(self, arr):  # interpolating missing landmarks\n",
    "\n",
    "        arr_x = arr[:, :, 0]\n",
    "        arr_x = pd.DataFrame(arr_x)\n",
    "        arr_x = arr_x.interpolate(method=\"linear\", limit_direction=\"both\").to_numpy()\n",
    "\n",
    "        arr_y = arr[:, :, 1]\n",
    "        arr_y = pd.DataFrame(arr_y)\n",
    "        arr_y = arr_y.interpolate(method=\"linear\", limit_direction=\"both\").to_numpy()\n",
    "\n",
    "        if np.count_nonzero(~np.isnan(arr_x)) == 0:\n",
    "            arr_x = np.zeros(arr_x.shape)\n",
    "        if np.count_nonzero(~np.isnan(arr_y)) == 0:\n",
    "            arr_y = np.zeros(arr_y.shape)\n",
    "\n",
    "        arr_x = arr_x * self.frame_width\n",
    "        arr_y = arr_y * self.frame_length\n",
    "\n",
    "        result = np.stack((arr_x, arr_y), axis=-1)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def align_hand_to_pose(self, hand_keypoints, wrist_pose, thumb_pose):\n",
    "        \"\"\"\n",
    "        Aligns hand keypoints to pose wrist and thumb points, maintaining relative distances between keypoints.\n",
    "\n",
    "        Parameters:\n",
    "            hand_keypoints (np.ndarray): Array of shape (21, 2) representing hand keypoints.\n",
    "            wrist_pose (np.ndarray): Array of shape (2,) representing pose wrist point.\n",
    "            thumb_pose (np.ndarray): Array of shape (2,) representing pose thumb point.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: New hand keypoints array of shape (21, 2) with adjusted positions.\n",
    "        \"\"\"\n",
    "        # Create a new array to store the transformed hand keypoints\n",
    "        aligned_hand_keypoints = np.copy(hand_keypoints)\n",
    "\n",
    "        # Move WRIST to align with the pose wrist\n",
    "        aligned_hand_keypoints[0] = wrist_pose\n",
    "\n",
    "        # Calculate the vector from current WRIST to THUMB_TIP\n",
    "        thumb_vector = hand_keypoints[2] - hand_keypoints[0]\n",
    "\n",
    "        # Calculate the new vector from pose wrist to pose thumb\n",
    "        new_thumb_vector = thumb_pose - wrist_pose\n",
    "\n",
    "        # Calculate scaling factor and rotation\n",
    "        scale_factor = np.linalg.norm(new_thumb_vector) / np.linalg.norm(thumb_vector)\n",
    "        rotation_angle = np.arctan2(new_thumb_vector[1], new_thumb_vector[0]) - np.arctan2(thumb_vector[1], thumb_vector[0])\n",
    "        rotation_matrix = np.array([\n",
    "            [np.cos(rotation_angle), -np.sin(rotation_angle)],\n",
    "            [np.sin(rotation_angle),  np.cos(rotation_angle)]\n",
    "        ])\n",
    "\n",
    "        # Maintain interdistance for all keypoints relative to WRIST\n",
    "        for i in range(1, 21):\n",
    "            # Calculate relative position of each keypoint\n",
    "            relative_point = hand_keypoints[i] - hand_keypoints[0]\n",
    "\n",
    "            # Scale and rotate the relative position\n",
    "            new_point = rotation_matrix @ (relative_point * scale_factor)\n",
    "\n",
    "            # Update the keypoint position in the new array\n",
    "            aligned_hand_keypoints[i] = aligned_hand_keypoints[0] + new_point\n",
    "\n",
    "        return aligned_hand_keypoints\n",
    "\n",
    "    \n",
    "\n",
    "    def plot_hand(self, hand, img):\n",
    "        for coord in hand:\n",
    "            x, y = coord[0], coord[1]\n",
    "            cv2.circle(img, (int(x), int(y)), radius=5, color=(0,255,0), thickness=-1)\n",
    "        return img\n",
    "\n",
    "\n",
    "    def plot_pose(self, pose, img):\n",
    "        for coord in pose:\n",
    "            x, y = coord[0], coord[1]\n",
    "            cv2.circle(img, (int(x), int(y)), radius=5, color=(0,255,0), thickness=-1)\n",
    "        return img\n",
    "\n",
    "    def draw_connections(self, img, landmarks, connections, color=(255, 255, 255)):\n",
    "      for start_idx, end_idx in connections:\n",
    "            start = landmarks[start_idx]\n",
    "            end = landmarks[end_idx]\n",
    "            if not (np.isnan(start).any() or np.isnan(end).any()):\n",
    "                cv2.line(img, (int(start[0]), int(start[1])),\n",
    "                        (int(end[0]), int(end[1])), color=color, thickness=2)\n",
    "      return img\n",
    "\n",
    "    def plot_landmark(self, left_hand, pose, right_hand):\n",
    "        mylist = []\n",
    "        for n in range(self.NUM_FRAMES):\n",
    "            width, height = map(int, self.resolution.split(\":\"))\n",
    "            image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "            image = self.plot_hand(hand=left_hand[n], img=image)\n",
    "            image = self.plot_hand(hand=right_hand[n], img=image)\n",
    "            image = self.plot_pose(pose=pose[n], img=image)\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            pose_connections = [\n",
    "                (0, 1), (1, 2), (2, 3), (3, 7), (0, 4), (4, 5), (5, 6), (6, 8), (9, 10),\n",
    "                (11, 12), (11, 13), (13, 15), (15, 17), (15, 19), (17, 19),\n",
    "                (12, 14), (14, 16), (16, 18), (16, 20), (18, 20),\n",
    "                (11, 23), (12, 24), (23, 24), (15, 21), (16, 22)\n",
    "            ]\n",
    "\n",
    "            hand_connections = [\n",
    "                (0, 1), (1, 2), (2, 3), (3, 4),       # Thumb\n",
    "                (0, 5), (5, 6), (6, 7), (7, 8),       # Index finger\n",
    "                (0, 9), (9, 10), (10, 11), (11, 12),  # Middle finger\n",
    "                (0, 13), (13, 14), (14, 15), (15, 16),# Ring finger\n",
    "                (0, 17), (17, 18), (18, 19), (19, 20) # Pinky finger\n",
    "            ]\n",
    "            rgb_image = self.draw_connections(image, pose[n], pose_connections)\n",
    "            rgb_image = self.draw_connections(image, left_hand[n], hand_connections)\n",
    "            rgb_image = self.draw_connections(image, right_hand[n], hand_connections)\n",
    "            # plt.figure(figsize=(12,8))\n",
    "            # plt.imshow(rgb_image)\n",
    "            # plt.axis(\"off\")\n",
    "            # plt.show()\n",
    "            mylist.append(self.rescale(self.crop(torch.permute(torch.from_numpy(rgb_image).to(torch.uint8),(2,0,1)))))\n",
    "        return torch.stack(mylist)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        video_path = self.data[idx][0]\n",
    "        video_file_name = os.path.basename(video_path)  # Assigns the name of the video to the variable\n",
    "        output_video_path = os.path.join(self.temp_data_folder, video_file_name)  # creating a temp path in the temp_folder for saaving the frames of the video\n",
    "        os.makedirs(output_video_path, exist_ok=True)  # Creating a folder with the name of the video in the temp_data_folder to save the frames of the video\n",
    "\n",
    "        # Command to convert the video to frames\n",
    "        fallback_cmd = [\n",
    "                    'ffmpeg',\n",
    "                    '-i', video_path,\n",
    "                    '-vf', f'fps={self.fps},scale={self.resolution},format=yuv420p',\n",
    "                    '-q:v', '2',\n",
    "                    os.path.join(output_video_path, 'img_%05d.jpg')\n",
    "        ]\n",
    "        subprocess.run(fallback_cmd, check=True, stderr=subprocess.PIPE)\n",
    "\n",
    "        video_frames = []\n",
    "        left_hand, right_hand, pose = [[],[]], [[],[]], [[],[]]\n",
    "\n",
    "        if np.random.randint(low=1, high=100, size=None, dtype=int) <= self.flip_prob:\n",
    "            flip=True\n",
    "        else:\n",
    "            flip=False\n",
    "\n",
    "        video_images = os.listdir(output_video_path)\n",
    "\n",
    "        # Setting variables for the range in for loop\n",
    "        starting_frame = 0\n",
    "        step_size = len(video_images)//self.NUM_FRAMES\n",
    "        ending_frame = len(video_images)-len(video_images)%self.NUM_FRAMES  # Subtraction to remove the remainder so that we run the for loop extra\n",
    "\n",
    "        # Selecting the NUM_FRAMES from the video\n",
    "        for n in range(starting_frame, ending_frame, step_size):\n",
    "            img = video_images[n]\n",
    "            img_path = os.path.join(output_video_path, img)\n",
    "\n",
    "            x_left_hand, y_left_hand, x_pose, y_pose, x_right_hand, y_right_hand = self.process_and_convert(img_path, flip)\n",
    "            left_hand[0].append(x_left_hand)\n",
    "            left_hand[1].append(y_left_hand)\n",
    "            right_hand[0].append(x_right_hand)\n",
    "            right_hand[1].append(y_right_hand)\n",
    "            pose[0].append(x_pose)\n",
    "            pose[1].append(y_pose)\n",
    "\n",
    "        left_hand= np.array(left_hand).transpose(1,2,0)\n",
    "        right_hand = np.array(right_hand).transpose(1, 2, 0)\n",
    "        pose = np.array(pose).transpose(1, 2, 0)\n",
    "\n",
    "        pose = self.interpolate(pose)\n",
    "        left_hand = self.interpolate(left_hand, )#pose[::,22])\n",
    "        right_hand = self.interpolate(right_hand, )#pose[::,21])\n",
    "\n",
    "        vid_tensor = self.plot_landmark(left_hand=left_hand, pose=pose, right_hand=right_hand)\n",
    "        shutil.rmtree(output_video_path)\n",
    "        label = self.data[idx][1]\n",
    "        return vid_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10f7ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoDataset(data, temp_data_folder=r\"D:\\ISL_temp\", NUM_FRAMES=24, transform_frame=None, video_fps=25, resolution='1280:720', flip_prob=0, resize=480, crop=720)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48bd0f5b-f90c-4038-8a3b-11d9c2a384ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "vid, label = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7fd654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# CNN Model\n",
    "class SignLanguageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, name='r3d'):\n",
    "        super(SignLanguageClassifier, self).__init__()\n",
    "\n",
    "        # model_name for save_checkpoint\n",
    "        self.name = name\n",
    "\n",
    "        # CNN (ResNet - 3D)\n",
    "        self.cnn = models.video.r3d_18(pretrained=True)  # Pretrained 3D ResNet\n",
    "        self.cnn.fc = nn.Identity()  # Remove final classification layer\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(512, num_classes)  # r3d_18 final layer outputs 512 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "\n",
    "        Args:\n",
    "        - x: Input tensor with shape [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        Returns:\n",
    "        - output: Model predictions with shape [Batch, Num_Classes]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, c, h, w = x.size()  # x: [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Reshape sequence as depth for 3D CNN\n",
    "        x = x.permute(0, 2, 1, 3, 4)  # [Batch, Channels, Depth (Seq_Len), Height, Width]\n",
    "\n",
    "        # Extract features using CNN\n",
    "        features = self.cnn(x)  # [Batch, 512, 1, 1, 1]\n",
    "        features = features.view(features.size(0), -1)  # Flatten: [Batch, 512]\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(features)  # [Batch, Num_Classes]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13fae7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\shobh\\AppData\\Local\\Temp\\ipykernel_12052\\1783129358.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(r\"C:\\Users\\shobh\\Downloads\\r3d_model=89.17_epoch=7_real.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SignLanguageClassifier(262).to(device)\n",
    "checkpoint = torch.load(\"path/to/the/saved/model\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21a60593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 15s\n",
      "Wall time: 2min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([True])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# vid, label = next(iter(dataloader))\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y = model(vid.to(torch.float32).to(device))\n",
    "y.argmax(dim=1).item() == label.to(device).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64a4e280-b33c-4112-8d38-a09f18038a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71b2f85f-8032-436e-9f3f-6aa87fdcef42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6550e-05)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.softmax(dim=1)[0][110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe7d2f-c3f7-459d-93c5-dc563687ad9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
