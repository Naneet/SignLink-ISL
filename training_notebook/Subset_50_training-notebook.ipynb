{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 18 not upgraded.\n",
      "Need to get 175 kB of archives.\n",
      "After this operation, 386 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 unzip amd64 6.0-26ubuntu3.2 [175 kB]\n",
      "Fetched 175 kB in 0s (815 kB/s)[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package unzip.\n",
      "(Reading database ... 16763 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-26ubuntu3.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking unzip (6.0-26ubuntu3.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up unzip (6.0-26ubuntu3.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  251M  100  251M    0     0  23.4M      0  0:00:10  0:00:10 --:--:-- 27.1M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  246M  100  246M    0     0  22.8M      0  0:00:10  0:00:10 --:--:-- 27.5M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  134M  100  134M    0     0  20.6M      0  0:00:06  0:00:06 --:--:-- 25.9M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  249M  100  249M    0     0  28.7M      0  0:00:08  0:00:08 --:--:-- 35.8M\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o include-interpolated-cv-300p-random-rotate.zip https://www.kaggle.com/api/v1/datasets/download/riceyy/include-interpolated-cv-300p-random-rotate\n",
    "!curl -L -o include-interpolated-cv-300p-random-crop.zip https://www.kaggle.com/api/v1/datasets/download/riceyy/include-interpolated-cv-300p-random-crop\n",
    "!curl -L -o include-interpolated-300p.zip https://www.kaggle.com/api/v1/datasets/download/naneet1/include-interpolated-300p\n",
    "!curl -L -o include-interpolated-cv-300p-double-madness.zip https://www.kaggle.com/api/v1/datasets/download/riceyy/include-interpolated-cv-300p-double-madness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  include-interpolated-cv-300p-random-rotate.zip\n",
      "  inflating: Test_data/tensor_0.pt   \n",
      "  inflating: Test_data/tensor_1.pt   \n",
      "  inflating: Test_data/tensor_10.pt  \n",
      "  inflating: Test_data/tensor_11.pt  \n",
      "  inflating: Test_data/tensor_12.pt  \n",
      "  inflating: Test_data/tensor_13.pt  \n",
      "  inflating: Test_data/tensor_14.pt  \n",
      "  inflating: Test_data/tensor_2.pt   \n",
      "  inflating: Test_data/tensor_3.pt   \n",
      "  inflating: Test_data/tensor_4.pt   \n",
      "  inflating: Test_data/tensor_5.pt   \n",
      "  inflating: Test_data/tensor_6.pt   \n",
      "  inflating: Test_data/tensor_7.pt   \n",
      "  inflating: Test_data/tensor_8.pt   \n",
      "  inflating: Test_data/tensor_9.pt   \n",
      "  inflating: Train_data/tensor_0.pt  \n",
      "  inflating: Train_data/tensor_1.pt  \n",
      "  inflating: Train_data/tensor_10.pt  \n",
      "  inflating: Train_data/tensor_11.pt  \n",
      "  inflating: Train_data/tensor_12.pt  \n",
      "  inflating: Train_data/tensor_13.pt  \n",
      "  inflating: Train_data/tensor_14.pt  \n",
      "  inflating: Train_data/tensor_15.pt  \n",
      "  inflating: Train_data/tensor_16.pt  \n",
      "  inflating: Train_data/tensor_17.pt  \n",
      "  inflating: Train_data/tensor_18.pt  \n",
      "  inflating: Train_data/tensor_19.pt  \n",
      "  inflating: Train_data/tensor_2.pt  \n",
      "  inflating: Train_data/tensor_20.pt  \n",
      "  inflating: Train_data/tensor_21.pt  \n",
      "  inflating: Train_data/tensor_22.pt  \n",
      "  inflating: Train_data/tensor_23.pt  \n",
      "  inflating: Train_data/tensor_24.pt  \n",
      "  inflating: Train_data/tensor_25.pt  \n",
      "  inflating: Train_data/tensor_26.pt  \n",
      "  inflating: Train_data/tensor_27.pt  \n",
      "  inflating: Train_data/tensor_28.pt  \n",
      "  inflating: Train_data/tensor_29.pt  \n",
      "  inflating: Train_data/tensor_3.pt  \n",
      "  inflating: Train_data/tensor_30.pt  \n",
      "  inflating: Train_data/tensor_31.pt  \n",
      "  inflating: Train_data/tensor_32.pt  \n",
      "  inflating: Train_data/tensor_33.pt  \n",
      "  inflating: Train_data/tensor_34.pt  \n",
      "  inflating: Train_data/tensor_35.pt  \n",
      "  inflating: Train_data/tensor_36.pt  \n",
      "  inflating: Train_data/tensor_37.pt  \n",
      "  inflating: Train_data/tensor_38.pt  \n",
      "  inflating: Train_data/tensor_39.pt  \n",
      "  inflating: Train_data/tensor_4.pt  \n",
      "  inflating: Train_data/tensor_40.pt  \n",
      "  inflating: Train_data/tensor_41.pt  \n",
      "  inflating: Train_data/tensor_42.pt  \n",
      "  inflating: Train_data/tensor_43.pt  \n",
      "  inflating: Train_data/tensor_44.pt  \n",
      "  inflating: Train_data/tensor_45.pt  \n",
      "  inflating: Train_data/tensor_46.pt  \n",
      "  inflating: Train_data/tensor_47.pt  \n",
      "  inflating: Train_data/tensor_48.pt  \n",
      "  inflating: Train_data/tensor_49.pt  \n",
      "  inflating: Train_data/tensor_5.pt  \n",
      "  inflating: Train_data/tensor_50.pt  \n",
      "  inflating: Train_data/tensor_6.pt  \n",
      "  inflating: Train_data/tensor_7.pt  \n",
      "  inflating: Train_data/tensor_8.pt  \n",
      "  inflating: Train_data/tensor_9.pt  \n",
      "Archive:  include-interpolated-cv-300p-random-crop.zip\n",
      "replace Test_data/tensor_0.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n",
      "Archive:  include-interpolated-300p.zip\n",
      "  inflating: ISL_pickle/Test_data/tensor_0.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_1.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_10.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_11.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_12.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_13.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_14.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_2.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_3.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_4.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_5.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_6.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_7.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_8.pt  \n",
      "  inflating: ISL_pickle/Test_data/tensor_9.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_0.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_1.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_10.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_11.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_12.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_13.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_14.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_15.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_16.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_17.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_18.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_19.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_2.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_20.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_21.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_22.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_23.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_24.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_25.pt  \n",
      "  inflating: ISL_pickle/Train_data/tensor_26.pt  ^C\n",
      "Archive:  include-interpolated-cv-300p-double-madness.zip\n",
      "  inflating: Test_data/tensor_0.pt   ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip include-interpolated-cv-300p-random-rotate.zip -d /home/include-interpolated-cv-300p-random-rotate\n",
    "!unzip include-interpolated-cv-300p-random-crop.zip -d /home/include-interpolated-cv-300p-random-crop\n",
    "!unzip include-interpolated-300p.zip -d /home/include-interpolated-300p\n",
    "!unzip include-interpolated-cv-300p-double-madness.zip -d /home/include-interpolated-cv-300p-double-madness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# %matplotlib inline\n",
    "# from matplotlib import pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(path) -> torch.tensor:\n",
    "    return torch.load(path)\n",
    "\n",
    "def pickle_files_path_list(path) -> list:\n",
    "    path_list = []\n",
    "    for top, dirs, files in os.walk(path):\n",
    "        for nm in files:\n",
    "            path_list.append(os.path.join(top, nm))\n",
    "    return path_list\n",
    "\n",
    "def preload_tensors(path_list, device):\n",
    "    data = []\n",
    "    for path in path_list:\n",
    "        tensor = torch.load(path)  # Load tensor from disk\n",
    "        video, label = tensor['video'].to(device), tensor['label'].to(device)\n",
    "        data.append((video.to(torch.float16), label))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def save_checkpoint(state, filename=\"checkpoint.pth\"):\n",
    "    \"\"\"Save model and optimizer state to a file.\"\"\"\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename=\"checkpoint.pth\"):\n",
    "    \"\"\"Load model and optimizer state from a file.\"\"\"\n",
    "    checkpoint = torch.load(filename)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, device, optimizer, loss_fn, scheduler, save=False):\n",
    "        self.device = device\n",
    "        self.scaler = GradScaler()  # Automatic Mixed Precision\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler  # Add scheduler\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model = model\n",
    "        self.save = save\n",
    "\n",
    "    def train_step_pickle(self, epoch, preloaded_data):\n",
    "        self.model.train()\n",
    "        train_loss, total_correct, total_samples = 0, 0, 0\n",
    "\n",
    "        for X, y in preloaded_data:\n",
    "            # Forward pass with AMP\n",
    "            with autocast('cuda'):\n",
    "                y_pred = self.model(X)\n",
    "                loss = self.loss_fn(y_pred, y)\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            # Backward pass with scaling\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Accuracy computation\n",
    "            y_pred_class = torch.argmax(y_pred, dim=1)\n",
    "            total_correct += (y_pred_class == y).sum().item()\n",
    "            total_samples += y.size(0)\n",
    "\n",
    "        acc = total_correct * 100 / total_samples\n",
    "        train_loss /= len(preloaded_data)\n",
    "        print(f\"Epoch: {epoch} | Train Loss: {train_loss:.4f} | Accuracy: {acc:.2f}\")\n",
    "\n",
    "        # Step scheduler if it's not ReduceLROnPlateau\n",
    "        if not isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def test_step_pickle(self, epoch,preloaded_data):\n",
    "        self.model.eval()\n",
    "        test_loss, acc = 0, 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in preloaded_data:\n",
    "                # Use AMP for inference\n",
    "                with autocast('cuda'):\n",
    "                    y_pred = self.model(X)\n",
    "                    loss = self.loss_fn(y_pred, y)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                # Compute predictions and accuracy\n",
    "                y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "                total_correct += (y_pred_class == y).sum().item()\n",
    "                total_samples += y.size(0)\n",
    "\n",
    "            acc = total_correct * 100 / total_samples\n",
    "            test_loss = test_loss / len(preloaded_data)\n",
    "            print(f\"Epoch: {epoch} | Test Loss: {test_loss:.4f} | Accuracy: {acc:.2f}\")\n",
    "            \n",
    "            # Step scheduler if ReduceLROnPlateau\n",
    "            if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                self.scheduler.step(test_loss)  # Use validation loss as the metric\n",
    "            \n",
    "            if self.save:\n",
    "                print(f\"Saving model at epoch {epoch}...\")\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'best_metric': acc\n",
    "                }, filename=f\"resnet_pretrained_acc={acc:.2f}_loss={loss:.4f}_{epoch=}_real.pth\")\n",
    "            print(\"************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_547/4016792477.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tensor = torch.load(path)  # Load tensor from disk\n"
     ]
    }
   ],
   "source": [
    "train_preloaded = []\n",
    "train_path = ['/home/include-interpolated-cv-300p-random-crop/Train_data','/home/include-interpolated-300p/ISL_pickle/Train_data',\n",
    "              '/home/include-interpolated-cv-300p-double-madness/Train_data','/home/include-interpolated-cv-300p-random-rotate/Train_data']\n",
    "\n",
    "for path in train_path:\n",
    "    train_preloaded.extend(preload_tensors(pickle_files_path_list(path), device='cuda:0'))\n",
    "\n",
    "\n",
    "test_preloaded_all = []\n",
    "test_path_all = ['/home/include-interpolated-cv-300p-random-crop/Test_data','/home/include-interpolated-300p/ISL_pickle/Test_data',\n",
    "                '/home/include-interpolated-cv-300p-double-madness/Test_data','/home/include-interpolated-cv-300p-random-rotate/Test_data']\n",
    "\n",
    "for path in test_path_all:\n",
    "    test_preloaded_all.extend(preload_tensors(pickle_files_path_list(path), device='cuda:0'))\n",
    "\n",
    "test_preloaded_unaug = preload_tensors(pickle_files_path_list('/home/include-interpolated-300p/ISL_pickle/Test_data'), device='cuda:0')\n",
    "\n",
    "\n",
    "\n",
    "# train_preloaded = preload_tensors(pickle_files_path_list('/kaggle/input/include-interpolated-cv-300p-random-crop/Train_data'), device='cuda:0')\n",
    "# test_preloaded = preload_tensors(pickle_files_path_list('/kaggle/input/include-interpolated-cv-300p-random-crop/Test_data'), device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 60, 15)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_preloaded), len(test_preloaded_all), len(test_preloaded_unaug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# CNN Model\n",
    "class r3d(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(r3d, self).__init__()\n",
    "\n",
    "        # CNN (ResNet - 3D)\n",
    "        self.cnn = models.video.r3d_18(pretrained=True)  # Pretrained 3D ResNet\n",
    "        self.cnn.fc = nn.Identity()  # Remove final classification layer\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(512, num_classes)  # r3d_18 final layer outputs 512 features\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "\n",
    "        Args:\n",
    "        - x: Input tensor with shape [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        Returns:\n",
    "        - output: Model predictions with shape [Batch, Num_Classes]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, c, h, w = x.size()  # x: [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Reshape sequence as depth for 3D CNN\n",
    "        x = x.permute(0, 2, 1, 3, 4)  # [Batch, Channels, Depth (Seq_Len), Height, Width]\n",
    "\n",
    "        # Extract features using CNN\n",
    "        features = self.cnn(x)  # [Batch, 512, 1, 1, 1]\n",
    "        features = features.view(features.size(0), -1)  # Flatten: [Batch, 512]\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(features)  # [Batch, Num_Classes]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# CNN + GRU Model with Dropouts\n",
    "class r18gru(nn.Module):\n",
    "    def __init__(self, num_classes, dropout):\n",
    "        super(r18gru, self).__init__()\n",
    "\n",
    "        # Pre-trained CNN (e.g., ResNet)\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])  # Remove the final classification layer\n",
    "\n",
    "        # Dropout after CNN features\n",
    "        self.cnn_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # GRU for temporal modeling\n",
    "        self.gru = nn.GRU(input_size=512, hidden_size=256, num_layers=2, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "        # Dropout before final FC layer\n",
    "        self.fc_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "\n",
    "        Args:\n",
    "        - x: Input tensor with shape [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        Returns:\n",
    "        - output: Model predictions with shape [Batch, Num_Classes]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, c, h, w = x.size()  # x: [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Reshape input for CNN processing\n",
    "        x = x.view(batch_size * seq_len, c, h, w)  # Combine Batch and Sequence dimensions: [Batch * Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Extract features using CNN\n",
    "        features = self.cnn(x)  # [Batch * Sequence, 512, 1, 1] (ResNet-18 output)\n",
    "        features = features.view(features.size(0), -1)  # Flatten: [Batch * Sequence, 512]\n",
    "\n",
    "        # Apply CNN dropout\n",
    "        features = self.cnn_dropout(features)\n",
    "\n",
    "        # Reshape features back to sequence format for GRU\n",
    "        features = features.view(batch_size, seq_len, -1)  # [Batch, Sequence, Features]\n",
    "\n",
    "        # Pass the sequence of features through the GRU\n",
    "        gru_out, _ = self.gru(features)  # [Batch, Sequence, Hidden_Size]\n",
    "\n",
    "        # Take the output of the last time step\n",
    "        final_output = gru_out[:, -1, :]  # [Batch, Hidden_Size]\n",
    "\n",
    "        # Apply dropout before the fully connected layer\n",
    "        final_output = self.fc_dropout(final_output)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(final_output)  # [Batch, Num_Classes]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# CNN + LSTM Model\n",
    "class r18lstm(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(r18lstm, self).__init__()\n",
    "\n",
    "        # Pre-trained CNN (e.g., ResNet)\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])  # Remove the final classification layer\n",
    "\n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "\n",
    "        Args:\n",
    "        - x: Input tensor with shape [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        Returns:\n",
    "        - output: Model predictions with shape [Batch, Num_Classes]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, c, h, w = x.size()  # x: [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Reshape input for CNN processing\n",
    "        x = x.view(batch_size * seq_len, c, h, w)  # Combine Batch and Sequence dimensions: [Batch * Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Extract features using CNN\n",
    "        features = self.cnn(x)  # [Batch * Sequence, 512, 1, 1] (ResNet-18 output)\n",
    "        features = features.view(features.size(0), -1)  # Flatten: [Batch * Sequence, 512]\n",
    "\n",
    "        # Reshape features back to sequence format for LSTM\n",
    "        features = features.view(batch_size, seq_len, -1)  # [Batch, Sequence, Features]\n",
    "\n",
    "        # Pass the sequence of features through the LSTM\n",
    "        lstm_out, _ = self.lstm(features)  # [Batch, Sequence, Hidden_Size]\n",
    "\n",
    "        # Take the output of the last time step\n",
    "        final_output = lstm_out[:, -1, :]  # [Batch, Hidden_Size]\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(final_output)  # [Batch, Num_Classes]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# CNN + GRU Model with Dropouts\n",
    "class mv3gru(nn.Module):\n",
    "    def __init__(self, num_classes, dropout):\n",
    "        super(mv3gru, self).__init__()\n",
    "\n",
    "        # Pre-trained CNN (e.g., ResNet)\n",
    "        self.cnn = models.mobilenet_v3_large(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])  # Remove the final classification layer\n",
    "\n",
    "        # Dropout after CNN features\n",
    "        self.cnn_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # GRU for temporal modeling\n",
    "        self.gru = nn.GRU(input_size=960, hidden_size=256, num_layers=2, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "        # Dropout before final FC layer\n",
    "        self.fc_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "\n",
    "        Args:\n",
    "        - x: Input tensor with shape [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        Returns:\n",
    "        - output: Model predictions with shape [Batch, Num_Classes]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, c, h, w = x.size()  # x: [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Reshape input for CNN processing\n",
    "        x = x.view(batch_size * seq_len, c, h, w)  # Combine Batch and Sequence dimensions: [Batch * Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Extract features using CNN\n",
    "        features = self.cnn(x)  # [Batch * Sequence, 512, 1, 1] (ResNet-18 output)\n",
    "        features = features.view(features.size(0), -1)  # Flatten: [Batch * Sequence, 512]\n",
    "\n",
    "        # Apply CNN dropout\n",
    "        features = self.cnn_dropout(features)\n",
    "\n",
    "        # Reshape features back to sequence format for GRU\n",
    "        features = features.view(batch_size, seq_len, -1)  # [Batch, Sequence, Features]\n",
    "\n",
    "        # Pass the sequence of features through the GRU\n",
    "        gru_out, _ = self.gru(features)  # [Batch, Sequence, Hidden_Size]\n",
    "\n",
    "        # Take the output of the last time step\n",
    "        final_output = gru_out[:, -1, :]  # [Batch, Hidden_Size]\n",
    "\n",
    "        # Apply dropout before the fully connected layer\n",
    "        final_output = self.fc_dropout(final_output)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(final_output)  # [Batch, Num_Classes]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# CNN + GRU Model with Dropouts\n",
    "class r50gru(nn.Module):\n",
    "    def __init__(self, num_classes, dropout):\n",
    "        super(r50gru, self).__init__()\n",
    "\n",
    "        # Pre-trained CNN (e.g., ResNet)\n",
    "        self.cnn = models.resnet50(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])  # Remove the final classification layer\n",
    "\n",
    "        # Dropout after CNN features\n",
    "        self.cnn_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # GRU for temporal modeling\n",
    "        self.gru = nn.GRU(input_size=2048, hidden_size=256, num_layers=2, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "        # Dropout before final FC layer\n",
    "        self.fc_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "\n",
    "        Args:\n",
    "        - x: Input tensor with shape [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        Returns:\n",
    "        - output: Model predictions with shape [Batch, Num_Classes]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, c, h, w = x.size()  # x: [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Reshape input for CNN processing\n",
    "        x = x.view(batch_size * seq_len, c, h, w)  # Combine Batch and Sequence dimensions: [Batch * Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Extract features using CNN\n",
    "        features = self.cnn(x)  # [Batch * Sequence, 512, 1, 1] (ResNet-18 output)\n",
    "        features = features.view(features.size(0), -1)  # Flatten: [Batch * Sequence, 512]\n",
    "\n",
    "        # Apply CNN dropout\n",
    "        features = self.cnn_dropout(features)\n",
    "\n",
    "        # Reshape features back to sequence format for GRU\n",
    "        features = features.view(batch_size, seq_len, -1)  # [Batch, Sequence, Features]\n",
    "\n",
    "        # Pass the sequence of features through the GRU\n",
    "        gru_out, _ = self.gru(features)  # [Batch, Sequence, Hidden_Size]\n",
    "\n",
    "        # Take the output of the last time step\n",
    "        final_output = gru_out[:, -1, :]  # [Batch, Hidden_Size]\n",
    "\n",
    "        # Apply dropout before the fully connected layer\n",
    "        final_output = self.fc_dropout(final_output)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(final_output)  # [Batch, Num_Classes]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# CNN + GRU Model with Dropouts\n",
    "class r50lstm(nn.Module):\n",
    "    def __init__(self, num_classes, dropout):\n",
    "        super(r50lstm, self).__init__()\n",
    "\n",
    "        # Pre-trained CNN (e.g., ResNet)\n",
    "        self.cnn = models.resnet50(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(self.cnn.children())[:-1])  # Remove the final classification layer\n",
    "\n",
    "        # Dropout after CNN features\n",
    "        self.cnn_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # GRU for temporal modeling\n",
    "        self.gru = nn.LSTM(input_size=2048, hidden_size=256, num_layers=2, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "        # Dropout before final FC layer\n",
    "        self.fc_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "\n",
    "        Args:\n",
    "        - x: Input tensor with shape [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        Returns:\n",
    "        - output: Model predictions with shape [Batch, Num_Classes]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, c, h, w = x.size()  # x: [Batch, Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Reshape input for CNN processing\n",
    "        x = x.view(batch_size * seq_len, c, h, w)  # Combine Batch and Sequence dimensions: [Batch * Sequence, Channels, Height, Width]\n",
    "\n",
    "        # Extract features using CNN\n",
    "        features = self.cnn(x)  # [Batch * Sequence, 512, 1, 1] (ResNet-18 output)\n",
    "        features = features.view(features.size(0), -1)  # Flatten: [Batch * Sequence, 512]\n",
    "\n",
    "        # Apply CNN dropout\n",
    "        features = self.cnn_dropout(features)\n",
    "\n",
    "        # Reshape features back to sequence format for GRU\n",
    "        features = features.view(batch_size, seq_len, -1)  # [Batch, Sequence, Features]\n",
    "\n",
    "        # Pass the sequence of features through the GRU\n",
    "        gru_out, _ = self.gru(features)  # [Batch, Sequence, Hidden_Size]\n",
    "\n",
    "        # Take the output of the last time step\n",
    "        final_output = gru_out[:, -1, :]  # [Batch, Hidden_Size]\n",
    "\n",
    "        # Apply dropout before the fully connected layer\n",
    "        final_output = self.fc_dropout(final_output)\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(final_output)  # [Batch, Num_Classes]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T13:17:26.922664Z",
     "iopub.status.busy": "2024-12-30T13:17:26.922355Z",
     "iopub.status.idle": "2024-12-30T13:17:27.020217Z",
     "shell.execute_reply": "2024-12-30T13:17:27.018950Z",
     "shell.execute_reply.started": "2024-12-30T13:17:26.922640Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '/kaggle/working'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2e4ec6640a58>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    729\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m                     \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    727\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0mfd_closed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                     \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '/kaggle/working'"
     ]
    }
   ],
   "source": [
    "# shutil.rmtree('/kaggle/working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = r18gru(num_classes=50,dropout=0.1).to(device)\n",
    "model = r3d(50).to(device)\n",
    "# model = r50lstm(50).to(device, dropout=0.1)\n",
    "# model = r50gru(50).to(device)\n",
    "# model = r18lstm(50).to(device)\n",
    "# model = mv3gru(50).to(device)\n",
    "# model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Define the scheduler without the verbose parameter\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    min_lr=1e-6  # Set a minimum learning rate\n",
    ")\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Epoch: 0 | Train Loss: 2.7071 | Accuracy: 48.76\n",
      "Test all:\n",
      "Epoch: 0 | Test Loss: 2.2607 | Accuracy: 55.84\n",
      "Saving model at epoch 0...\n",
      "************************\n",
      "Train:\n",
      "Epoch: 1 | Train Loss: 1.0202 | Accuracy: 92.79\n",
      "Test all:\n",
      "Epoch: 1 | Test Loss: 1.7306 | Accuracy: 63.96\n",
      "Saving model at epoch 1...\n",
      "************************\n",
      "Train:\n",
      "Epoch: 2 | Train Loss: 0.3309 | Accuracy: 99.28\n",
      "Test all:\n",
      "Epoch: 2 | Test Loss: 1.4053 | Accuracy: 65.26\n",
      "Saving model at epoch 2...\n",
      "************************\n",
      "Train:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreloaded_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_preloaded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest all:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtest_step_pickle(epoch\u001b[38;5;241m=\u001b[39mepoch, preloaded_data\u001b[38;5;241m=\u001b[39mtest_preloaded_all)\n",
      "Cell \u001b[0;32mIn[15], line 24\u001b[0m, in \u001b[0;36mTrainer.train_step_pickle\u001b[0;34m(self, epoch, preloaded_data)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Backward pass with scaling\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/amp/grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/amp/grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(device=device,optimizer=optimizer,loss_fn=loss_fn,save=True,scheduler=scheduler, model=model)\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    print(\"Train:\")\n",
    "    trainer.train_step_pickle(epoch=epoch, preloaded_data=train_preloaded)\n",
    "    print(\"Test all:\")\n",
    "    trainer.test_step_pickle(epoch=epoch, preloaded_data=test_preloaded_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Confusion Matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_preds = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for X, y in test_preloaded:\n",
    "    X, y = X.to(device).float(), y.to(device)\n",
    "\n",
    "\n",
    "    y_pred = model(X)\n",
    "    y_pred_class = y_pred.argmax(dim=1)\n",
    "\n",
    "    y_true.extend(y.cpu().numpy())\n",
    "    y_preds.extend(y_pred_class.cpu().numpy())\n",
    "plt.figure(figsize=(1000, 1000))\n",
    "cm = confusion_matrix(y_true, y_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(50))\n",
    "disp.plot(cmap=\"viridis\")  # Customize colormap if needed\n",
    "plt.xticks(fontsize=6, rotation=45, ha=\"right\")\n",
    "plt.yticks(fontsize=6)\n",
    "plt.show()\n",
    "import seaborn as sns\n",
    "def cm_disp(cm):\n",
    "    mask = np.zeros_like(cm)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    sns.heatmap(cm, mask=mask, vmax=.3, square=True,  cmap=\"YlGnBu\")\n",
    "    plt.show()\n",
    "cm_disp(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Confusion Matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = []\n",
    "y_preds = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for X, y in train_preloaded:\n",
    "    X, y = X.to(device).float(), y.to(device)\n",
    "\n",
    "\n",
    "    y_pred = model(X)\n",
    "    y_pred_class = y_pred.argmax(dim=1)\n",
    "\n",
    "    y_true.extend(y.cpu().numpy())\n",
    "    y_preds.extend(y_pred_class.cpu().numpy())\n",
    "plt.figure(figsize=(1000, 1000))\n",
    "cm = confusion_matrix(y_true, y_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(50))\n",
    "disp.plot(cmap=\"viridis\")  # Customize colormap if needed\n",
    "plt.xticks(fontsize=6, rotation=45, ha=\"right\")\n",
    "plt.yticks(fontsize=6)\n",
    "plt.show()\n",
    "import seaborn as sns\n",
    "def cm_disp(cm):\n",
    "    mask = np.zeros_like(cm)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    sns.heatmap(cm, mask=mask, vmax=.3, square=True,  cmap=\"YlGnBu\")\n",
    "    plt.show()\n",
    "cm_disp(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('resnet_pretrained_acc=88.74_loss=0.2813_epoch=30_real.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T05:19:25.458788Z",
     "iopub.status.busy": "2024-12-31T05:19:25.458502Z",
     "iopub.status.idle": "2024-12-31T05:19:31.839895Z",
     "shell.execute_reply": "2024-12-31T05:19:31.838980Z",
     "shell.execute_reply.started": "2024-12-31T05:19:25.458767Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|| 44.7M/44.7M [00:00<00:00, 185MB/s] \n",
      "<ipython-input-9-7ecaffe87109>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint1 = torch.load('/kaggle/input/models/resnet_pretrained_GRU_acc88.74_loss0.2813_epoch30_real.pth')\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n",
      "100%|| 127M/127M [00:00<00:00, 192MB/s]  \n",
      "<ipython-input-9-7ecaffe87109>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint2 = torch.load('/kaggle/input/models/r3d_18_pretrained_acc89.18_loss0.7433_epoch15_real.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load checkpoint for model1 (r18gru)\n",
    "model1 = r18gru(num_classes=50, dropout=0.1).to(device)\n",
    "model1 = nn.DataParallel(model1)  # Distribute model across GPUs\n",
    "\n",
    "checkpoint1 = torch.load('/kaggle/input/models/resnet_pretrained_GRU_acc88.74_loss0.2813_epoch30_real.pth')\n",
    "model1.load_state_dict(checkpoint1['model_state_dict'])\n",
    "\n",
    "# Load checkpoint for model2 (r3d_18)\n",
    "model2 = r3d(50).to(device)\n",
    "model2 = nn.DataParallel(model2)\n",
    "\n",
    "checkpoint2 = torch.load('/kaggle/input/models/r3d_18_pretrained_acc89.18_loss0.7433_epoch15_real.pth')\n",
    "model2.load_state_dict(checkpoint2['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T05:34:48.334840Z",
     "iopub.status.busy": "2024-12-31T05:34:48.334546Z",
     "iopub.status.idle": "2024-12-31T05:35:06.770277Z",
     "shell.execute_reply": "2024-12-31T05:35:06.769329Z",
     "shell.execute_reply.started": "2024-12-31T05:34:48.334819Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    for X, y in test_preloaded:\n",
    "        y_pred_batch = 0\n",
    "        # Convert the labels and predictions to numpy arrays\n",
    "        y_true_batch = y.tolist()\n",
    "        #y_pred_batch += model1(X.to(torch.float32))\n",
    "        y_pred_batch += model2(X.to(torch.float32))\n",
    "        #y_pred_batch /= 2\n",
    "        # Concatenate the current batch predictions and labels\n",
    "        y_true.extend(y_true_batch)  # Wrap arrays in a tuple\n",
    "        y_pred.extend(y_pred_batch.argmax(dim=1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T05:35:06.771694Z",
     "iopub.status.busy": "2024-12-31T05:35:06.771431Z",
     "iopub.status.idle": "2024-12-31T05:35:06.802868Z",
     "shell.execute_reply": "2024-12-31T05:35:06.802072Z",
     "shell.execute_reply.started": "2024-12-31T05:35:06.771672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro\n",
      "F1 Score: 0.2641\n",
      "Precision: 0.2641\n",
      "Recall: 0.2641\n",
      "***************\n",
      "macro\n",
      "F1 Score: 0.2501\n",
      "Precision: 0.3475\n",
      "Recall: 0.2630\n",
      "***************\n",
      "weighted\n",
      "F1 Score: 0.2588\n",
      "Precision: 0.3621\n",
      "Recall: 0.2641\n",
      "***************\n",
      "Acc = 0.2641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "types= ['micro', 'macro', 'weighted']\n",
    "for ty in types:\n",
    "    print(ty)\n",
    "    # Assuming y_true and y_pred are your ground truth and predictions\n",
    "    precision = precision_score(y_true, y_pred, average=ty)  # Use 'binary' for binary classification\n",
    "    recall = recall_score(y_true, y_pred, average=ty)        # Use 'macro' or 'micro' for multiclass if needed\n",
    "    f1 = f1_score(y_true, y_pred, average=ty) # 'weighted' handles class imbalance\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(\"***************\")\n",
    "print(f\"Acc = {accuracy_score(y_true, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6397140,
     "sourceId": 10331646,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6400606,
     "sourceId": 10336643,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6402289,
     "sourceId": 10339311,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6402406,
     "sourceId": 10339527,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
